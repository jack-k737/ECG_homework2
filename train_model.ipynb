{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we will apply an CNN to extract features and implement a classification task.\n",
    "# Firstly, we should build the model by PyTorch. We provide a baseline model here.\n",
    "# You can use your own model for better performance\n",
    "class Doubleconv_33(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_33, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_35(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_35, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_37(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_37, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Tripleconv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Tripleconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, ch_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc(input)\n",
    "\n",
    "\n",
    "class Mscnn(nn.Module):\n",
    "    # TODO: Build a better model\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Mscnn, self).__init__()\n",
    "        self.conv11 = Doubleconv_33(ch_in, 64)\n",
    "        self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv12 = Doubleconv_33(64, 128)\n",
    "        self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv13 = Tripleconv(128, 256)\n",
    "        self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv14 = Tripleconv(256, 512)\n",
    "        self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv15 = Tripleconv(512, 512)\n",
    "        self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.out = MLP(512*27, ch_out)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        c11 = self.conv11(x)\n",
    "        p11 = self.pool11(c11)\n",
    "        c12 = self.conv12(p11)\n",
    "        p12 = self.pool12(c12)\n",
    "        c13 = self.conv13(p12)\n",
    "        p13 = self.pool13(c13)\n",
    "        c14 = self.conv14(p13)\n",
    "        p14 = self.pool14(c14)\n",
    "        c15 = self.conv15(p14)\n",
    "        p15 = self.pool15(c15)\n",
    "        merge = p15.view(p15.size()[0], -1) \n",
    "        output = self.out(merge)\n",
    "        output = F.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to construct the data loader for training. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.io as io\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Random clipping has been implemented, \n",
    "# and you need to add noise and random scaling. \n",
    "# Generally, the scaling should be done before the crop.\n",
    "# In general, do not add scaling and noise enhancement options during testing\n",
    "\n",
    "class ECG_dataset(Dataset):\n",
    "\n",
    "    def __init__(self,base_file,cv=0, is_train=True):\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.file_list=[]\n",
    "        self.base_file=base_file\n",
    "        \n",
    "        for i in range(5):\n",
    "            data=pd.read_csv(base_file+'/cv/cv'+str(i)+'.csv')\n",
    "            self.file_list.append(data.to_numpy())\n",
    "        self.file=None\n",
    "        if is_train:\n",
    "            del self.file_list[cv]\n",
    "            self.file=self.file_list[0]\n",
    "            for i in range(1,4):\n",
    "                self.file=np.append(self.file,self.file_list[i],axis=0)\n",
    "        else:\n",
    "            self.file=self.file_list[cv]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.file.shape[0]\n",
    "    \n",
    "\n",
    "    def load_data(self,file_name,label):\n",
    "        #读取数据\n",
    "        mat_file = self.base_file+'/training2017/'+file_name+'.mat'\n",
    "        data = io.loadmat(mat_file)['val']\n",
    "        if label=='N':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='O':\n",
    "            one_hot=torch.tensor([0])\n",
    "        elif label=='A':\n",
    "            one_hot=torch.tensor([1])\n",
    "        elif label=='~':\n",
    "            one_hot=torch.tensor([0])\n",
    "        return data,one_hot\n",
    "\n",
    "\n",
    "    \n",
    "    def crop_padding(self,data,time):\n",
    "        #随机crop\n",
    "        if data.shape[0]<=time:\n",
    "            data=np.pad(data, (0,time-data.shape[0]), 'constant')\n",
    "        elif data.shape[0]>time:\n",
    "            end_index=data.shape[0]-time\n",
    "            start=np.random.randint(0, end_index)\n",
    "            data=data[start:start+time]\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def data_process(self,data):\n",
    "        # 学习论文以及数据集选择合适和采样率\n",
    "        # 并完成随机gaussian 噪声和随机时间尺度放缩\n",
    "        data=data[::3]\n",
    "        data=data-data.mean()\n",
    "        data=data/data.std()\n",
    "        data=self.crop_padding(data,2400)\n",
    "        data=torch.tensor(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name=self.file[idx][1]\n",
    "        label=self.file[idx][2]\n",
    "        data,one_hot=self.load_data(file_name,label)\n",
    "        data=self.data_process(data[0]).unsqueeze(0).float()\n",
    "        one_hot=one_hot.unsqueeze(0).float()\n",
    "        return data, one_hot,file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will build the pipeline for deep learning based training.\n",
    "# These functions may be useful :)\n",
    "def save_loss(fold, value):\n",
    "    path = 'loss' + str(fold) + '.txt'\n",
    "    file = open(path, mode='a+')\n",
    "    file.write(str(value)+'\\n')  \n",
    "    \n",
    "# We will use GPU if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mscnn(1, 1).to(device)   # ch_in, ch_out\n",
    "\n",
    "# Build pre-processing transformation \n",
    "# Note this pre-processing is in PyTorch\n",
    "x_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "])\n",
    "y_transforms = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# TODO: fine tune hyper-parameters\n",
    "batch_size = 64\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion2=torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_ecg_dataset = ECG_dataset(os.getcwd(), is_train=True)\n",
    "train_dataloader = DataLoader(train_ecg_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_ecg_dataset = ECG_dataset(os.getcwd(), is_train=False)\n",
    "test_dataloaders = DataLoader(test_ecg_dataset, batch_size=1)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss:0.31327844: 100%|██████████| 107/107 [01:26<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9019349164467898\n",
      "Accuracy: 0.9131964809384164\n",
      "Loss: 0.43253158677882814\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss:0.22510103: 100%|██████████| 107/107 [01:10<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tran_Accuracy: 0.9170331281149223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def validation(model,criterion,test_dataloaders,device):\n",
    "    # TODO: add more metrics for evaluation?\n",
    "    # Evaluate \n",
    "    model.eval()\n",
    "    predict = np.array([])\n",
    "    target = np.array([])\n",
    "    loss=0\n",
    "    step=0\n",
    "    with torch.no_grad():\n",
    "        for x, mask,name in test_dataloaders:\n",
    "            step += 1\n",
    "            mask=mask.to(device)\n",
    "            y = model(x.to(device))\n",
    "            loss +=criterion(y, mask.squeeze(2)).item()\n",
    "            y[y >= 0.5] = 1\n",
    "            y[y < 0.5] = 0\n",
    "            predict=np.append(predict,torch.squeeze(y).cpu().numpy())\n",
    "            target=np.append(target,torch.squeeze(mask).cpu().numpy())\n",
    "    acc = accuracy_score(target, predict)\n",
    "    print('Accuracy: {}'.format(acc))\n",
    "    print('Loss:', loss/step)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start training !\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "        predict = np.array([])\n",
    "        target = np.array([])\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        dt_size = len(train_dataloader.dataset)\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        process = tqdm(train_dataloader)\n",
    "        for x, y,name in process:\n",
    "            step += 1\n",
    "            inputs = x.to(device)\n",
    "            labels = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion2(outputs, labels.squeeze(2))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            process.set_description(\n",
    "                \"epoch: %d, train_loss:%0.8f\" % (epoch, epoch_loss / step)\n",
    "            )\n",
    "            outputs[outputs >= 0.5] = 1\n",
    "            outputs[outputs < 0.5] = 0\n",
    "            predict=np.append(predict,torch.squeeze(outputs).detach().cpu().numpy())\n",
    "            target=np.append(target,torch.squeeze(labels).detach().cpu().numpy())\n",
    "        epoch_loss /= step\n",
    "        acc = accuracy_score(target, predict)\n",
    "        print('tran_Accuracy: {}'.format(acc))\n",
    "        save_loss(10, epoch_loss)\n",
    "        validation(model,criterion2,test_dataloaders,device)\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'weights10_%d.pth' % (epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79495757624adbb2ec94ee769202e9d295b98634352b3931ea30316c5ea35353"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('pujin': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
